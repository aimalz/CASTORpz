{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aimalz/CASTORpz/blob/main/TheLastMetric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "P9ScjvW6K--M",
        "outputId": "22da04ef-7b20-415e-b501-569db7dad7d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "P9ScjvW6K--M",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Research/CASTORpz"
      ],
      "metadata": {
        "id": "AIt3xNrROef2",
        "outputId": "1f2a53ce-4279-40b3-d6be-0ef3793444dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AIt3xNrROef2",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ext_phot.fits  knn_colors.py  unperturbed_mags.fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from astropy.table import Table\n",
        "prepend = '/content/drive/MyDrive/Research/CASTORpz/'\n",
        "unp = Table.read(prepend+\"unperturbed_mags.fits\")\n",
        "print(len(unp))"
      ],
      "metadata": {
        "id": "tpLREN_EM6hg",
        "outputId": "ebfa4702-89a0-42ab-dd02-ba1565b5dfe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tpLREN_EM6hg",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "698211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install astropy pzflow"
      ],
      "metadata": {
        "id": "3u5XpOo5PdZF",
        "outputId": "0b62af93-d86d-4bc8-b864-97ae65c510e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3u5XpOo5PdZF",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: astropy in /usr/local/lib/python3.8/dist-packages (4.3.1)\n",
            "Collecting pzflow\n",
            "  Downloading pzflow-3.0.3-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from astropy) (1.21.6)\n",
            "Requirement already satisfied: pyerfa>=1.7.3 in /usr/local/lib/python3.8/dist-packages (from astropy) (2.0.0.1)\n",
            "Collecting optax<0.2.0,>=0.1.4\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.64.1 in /usr/local/lib/python3.8/dist-packages (from pzflow) (4.64.1)\n",
            "Collecting jax<0.5.0,>=0.4.2\n",
            "  Downloading jax-0.4.2.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaxlib<0.5.0,>=0.4.2\n",
            "  Downloading jaxlib-0.4.2-cp38-cp38-manylinux2014_x86_64.whl (71.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill<0.4.0,>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from pzflow) (0.3.6)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.8/dist-packages (from pzflow) (1.3.5)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.8/dist-packages (from jax<0.5.0,>=0.4.2->pzflow) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax<0.5.0,>=0.4.2->pzflow) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from optax<0.2.0,>=0.1.4->pzflow) (4.4.0)\n",
            "Collecting chex>=0.1.5\n",
            "  Downloading chex-0.1.6-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax<0.2.0,>=0.1.4->pzflow) (1.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1->pzflow) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1->pzflow) (2.8.2)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax<0.2.0,>=0.1.4->pzflow) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax<0.2.0,>=0.1.4->pzflow) (0.12.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1->pzflow) (1.15.0)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.2-py3-none-any.whl size=1363025 sha256=69e9dc2a1ad96b68f9e51f8db39c100c83ff747d5fa901bd9436e90f92759485\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/34/dd/2150fc0a0285435a6996e78998f124051996ef139e5ab7b310\n",
            "Successfully built jax\n",
            "Installing collected packages: jaxlib, jax, chex, optax, pzflow\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.25+cuda11.cudnn805\n",
            "    Uninstalling jaxlib-0.3.25+cuda11.cudnn805:\n",
            "      Successfully uninstalled jaxlib-0.3.25+cuda11.cudnn805\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.3.25\n",
            "    Uninstalling jax-0.3.25:\n",
            "      Successfully uninstalled jax-0.3.25\n",
            "Successfully installed chex-0.1.6 jax-0.4.2 jaxlib-0.4.2 optax-0.1.4 pzflow-3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pzflow import Flow\n",
        "import jax.numpy as jnp\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from astropy.table import Table\n",
        "from pzflow import Flow, FlowEnsemble\n",
        "from pzflow.distributions import Uniform\n",
        "from pzflow.bijectors import Chain, StandardScaler, NeuralSplineCoupling"
      ],
      "metadata": {
        "id": "A_-243XtQsTp"
      },
      "id": "A_-243XtQsTp",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "def getTrueY(test_cat, mag_col_names, y_col_name):\n",
        "    test_cat = Table(test_cat, masked=True, copy=True)\n",
        "    # remove nans\n",
        "    for col in mag_col_names:\n",
        "        test_cat[col].mask = np.isnan(test_cat[col].data) | test_cat[col].mask\n",
        "        test_cat = test_cat[~test_cat[col].mask] # then remove nans from test set\n",
        "            \n",
        "    true_y = test_cat[y_col_name]\n",
        "    return true_y.filled()"
      ],
      "metadata": {
        "id": "bOIAJea1Q8sv"
      },
      "id": "bOIAJea1Q8sv",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unp = Table.read(prepend+\"unperturbed_mags.fits\")\n",
        "mock = Table.read(prepend+\"ext_phot.fits\")\n",
        "mask = unp[\"Euclid_VIS_MAG\"] <= 24.5"
      ],
      "metadata": {
        "id": "lxPsZxjmRAqb"
      },
      "id": "lxPsZxjmRAqb",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unp = Table(unp.as_array().byteswap().newbyteorder('='))\n",
        "mock = Table(mock.as_array().byteswap().newbyteorder('='))\n",
        "mask = unp[\"Euclid_VIS_MAG\"] <= 24.5"
      ],
      "metadata": {
        "id": "Jfz5j4qoA58N"
      },
      "id": "Jfz5j4qoA58N",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CASTOR_baseline = mock[mask] #ignoring training data outside the Euclid flux limit "
      ],
      "metadata": {
        "id": "KlRbMi9lRFcl"
      },
      "id": "KlRbMi9lRFcl",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_phot = [\"ID\", \"photoz\",\n",
        "              \"LSST_g_MAG\", \"LSST_g_MAGERR\", \n",
        "              \"LSST_r_MAG\", \"LSST_r_MAGERR\",\n",
        "              \"LSST_i_MAG\", \"LSST_i_MAGERR\", \n",
        "              \"LSST_z_MAG\", \"LSST_z_MAGERR\",\n",
        "              \"castor_uv_MAG\", \"castor_uv_MAGERR\", \n",
        "              \"castor_u_MAG\", \"castor_u_MAGERR\", \n",
        "              \"castor_g_MAG\", \"castor_g_MAGERR\"]\n",
        "available_os = [\"baseline\"]\n",
        "names = [\n",
        "    \"baseline\",\n",
        "]"
      ],
      "metadata": {
        "id": "40UyfzNgRHtr"
      },
      "id": "40UyfzNgRHtr",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os_names = dict(zip(available_os, names))\n",
        "colors = [\"k\"] #, \"plum\", \"cornflowerblue\", \"#2ca02c\", \"gold\", \"tomato\"]\n",
        "os_colors = dict(zip(available_os, colors))"
      ],
      "metadata": {
        "id": "7_ElTN-FR281"
      },
      "id": "7_ElTN-FR281",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# put data in expected format for TLM\n",
        "\n",
        "LSST_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAG\")\n",
        "LSST_r_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAG\")\n",
        "LSST_i_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAG\")\n",
        "LSST_z_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAG\")\n",
        "\n",
        "LSST_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAGERR\")\n",
        "LSST_r_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAGERR\")\n",
        "LSST_i_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAGERR\")\n",
        "LSST_z_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAGERR\")\n",
        "\n",
        "CASTOR_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_g_MAG\")\n",
        "CASTOR_u_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_u_MAG\")\n",
        "CASTOR_uv_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_uv_MAG\")\n",
        "\n",
        "CASTOR_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_g_MAGERR\")\n",
        "CASTOR_u_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_u_MAGERR\")\n",
        "CASTOR_uv_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_uv_MAGERR\")\n",
        "\n",
        "ID = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"ID\")\n",
        "z_true = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"photoz\")\n",
        "\n"
      ],
      "metadata": {
        "id": "f8fFRKHGR7nR"
      },
      "id": "f8fFRKHGR7nR",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.DataFrame({'CASTOR_ID': data_array_sorted_CASTOR_wID[:,0] , 'error': data_array_sorted_CASTOR_wID[:,1], \n",
        "#                   'g-r': data_array_sorted_CASTOR_wID[:,2] - data_array_sorted_CASTOR_wID[:,3], \n",
        "#                   'r-i': data_array_sorted_CASTOR_wID[:,3] - data_array_sorted_CASTOR_wID[:,4], \n",
        "#                   'i-z': data_array_sorted_CASTOR_wID[:,4] - data_array_sorted_CASTOR_wID[:,5], \n",
        "#                   'true_z': data_array_sorted_CASTOR_wID[:,6]})\n",
        "\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    \n",
        "#     cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "#                              'LSST_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR, \n",
        "#                              'LSST_r_mag': LSST_r_mag, 'LSST_r_mag_ERR': LSST_r_mag_ERR, \n",
        "#                              'LSST_i_mag': LSST_i_mag, 'LSST_i_mag_ERR': LSST_i_mag_ERR, \n",
        "#                              'LSST_z_mag': LSST_z_mag, 'LSST_z_mag_ERR': LSST_z_mag_ERR, \n",
        "#                              'CASTOR_uv_mag': LSST_uv_mag, 'LSST_uv_mag_ERR': LSST_uv_mag_ERR, \n",
        "#                              'CASTOR_u_mag': LSST_u_mag, 'LSST_u_mag_ERR': LSST_u_mag_ERR, \n",
        "#               'CASTOR_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR})\n",
        "    \n",
        "    # this will need to change to accomodate multiple catalogs, e.g. LSST only or LSST + CASTOR\n",
        "    \n",
        "    cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "                             'r': LSST_r_mag,\n",
        "                             'g-r': LSST_g_mag - LSST_r_mag, \n",
        "                             'r-i': LSST_r_mag - LSST_i_mag, \n",
        "                             'i-z': LSST_i_mag - LSST_z_mag, \n",
        "                             'uv-u': CASTOR_uv_mag - CASTOR_u_mag, \n",
        "                             'u-g': CASTOR_u_mag - CASTOR_g_mag\n",
        "                      })\n",
        "    catalogs[os] = cat.dropna() "
      ],
      "metadata": {
        "id": "NhgJne8VSQuZ"
      },
      "id": "NhgJne8VSQuZ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still getting error here:\n",
        "\n",
        "\n",
        "```\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "ValueError                                Traceback (most recent call last)\n",
        "\n",
        "<ipython-input-33-db25b8cfd7b0> in <module>\n",
        "     27                              'u-g': CASTOR_u_mag - CASTOR_g_mag\n",
        "     28                       })\n",
        "---> 29     catalogs[os] = cat.dropna()\n",
        "\n",
        "12 frames\n",
        "\n",
        "/usr/local/lib/python3.8/dist-packages/pandas/core/array_algos/take.py in _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
        "    152         arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n",
        "    153     )\n",
        "--> 154     func(arr, indexer, out, fill_value)\n",
        "    155 \n",
        "    156     if flip_order:\n",
        "\n",
        "pandas/_libs/algos_take_helper.pxi in pandas._libs.algos.take_2d_axis0_int64_int64()\n",
        "\n",
        "ValueError: Big-endian buffer not supported on little-endian compiler\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "T0MYBOBuSe-t"
      },
      "id": "T0MYBOBuSe-t"
    },
    {
      "cell_type": "code",
      "source": [
        "# first I create a bijector chain\n",
        "# the first bijection is a standard scaler - but I'm not actually using it for standard scaling\n",
        "#     I set the mean and std so that it maps the redshift range (0, 3.2) onto (-5, 5), which is \n",
        "#     the domain of the NeuralSplineCoupling\n",
        "# the second bijection is a NeuralSplineCoupling. I told it to expect 6 conditions,\n",
        "#     which will be the r mag and the galaxy colors\n",
        "bijector = Chain(\n",
        "    StandardScaler(np.atleast_1d(1.6), np.atleast_1d(0.32)),\n",
        "    NeuralSplineCoupling(n_conditions=6)\n",
        ")\n",
        "\n",
        "# I set the latent distribution to a Uniform over (-5, 5)\n",
        "# this range was chosen to match the NeuralSplineCoupling domain\n",
        "# I chose a Uniform since all of the redshifts are drawn from a compact domain\n",
        "latent = Uniform(1, 5) # did the syntax here change?"
      ],
      "metadata": {
        "id": "CcBqdH2CSpjY"
      },
      "id": "CcBqdH2CSpjY",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary that will hold all the ensembles\n",
        "ensembles = dict()\n",
        "\n",
        "# create the baseline flows\n",
        "for os in available_os:\n",
        "    # the data column is the one that is sampled and transformed by the flow\n",
        "    data_columns = [\"z_true\"]\n",
        "    # the conditional columns are the columns that the flow is conditioned on\n",
        "    conditional_columns = [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\", \"uv-u\"] # different colors than LSST  \n",
        "    # save some info with the flow\n",
        "    info = f\"Models z_true conditioned on galaxy colors and r mag from os {os}. K=16\"\n",
        "    \n",
        "    # instantiate and save the flow\n",
        "    flowEns = FlowEnsemble(data_columns = data_columns, \n",
        "                           conditional_columns = conditional_columns,\n",
        "                           bijector = bijector,\n",
        "                           latent = latent,\n",
        "                           info = info,\n",
        "                           N = 10)\n",
        "    ensembles[os] = flowEns"
      ],
      "metadata": {
        "id": "LQUPqdiGSzyW",
        "outputId": "e61e001c-15dc-4bc3-f73c-3e439569a12d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LQUPqdiGSzyW",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for os, ens in ensembles.items():\n",
        "    \n",
        "    # get the data and make a train and test set\n",
        "    cat = catalogs[os]\n",
        "    cat_train = cat.sample(frac = 0.8)\n",
        "    cat_test = cat.drop(cat_train.index)\n",
        "    \n",
        "    # train the flow on the given learning rate schedule\n",
        "    loss1 = ens.train(cat_train, convolve_errs=True,\n",
        "                       epochs = 100, seed = 123)\n",
        "    loss2 = ens.train(cat_train, convolve_errs=True,\n",
        "                       epochs = 100, seed = 312)\n",
        "    loss3 = ens.train(cat_train, convolve_errs=True,\n",
        "                       epochs = 50, seed = 231)\n",
        "    \n",
        "    losses = {fname : # for each flow trained in the ensemble...\n",
        "                  [float(loss) # save the list of training losses\n",
        "                   for lossDict in [loss1, loss2, loss3]\n",
        "                   for loss in lossDict[fname]]\n",
        "              for fname in loss1}\n",
        "    \n",
        "    # print the train and test loss\n",
        "    train_loss = -np.mean(ens.log_prob(cat_train))\n",
        "    test_loss = -np.mean(ens.log_prob(cat_test))\n",
        "    print(os, train_loss, test_loss)\n",
        "    \n",
        "    # save the ensemble\n",
        "    ens.save(f\"trained_flows/pzflow_ensemble_for_{os}.pkl\")\n",
        "    # and the losses\n",
        "    with open(f\"trained_flows/losses_for_{os}.pkl\", \"wb\") as file:\n",
        "        pickle.dump({\"losses\": losses, \n",
        "                     \"train loss\": train_loss, \n",
        "                     \"test loss\": test_loss},\n",
        "                    file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7J0rqGeC_d4",
        "outputId": "d689bf1c-c98e-454b-9476-132d65a0193f"
      },
      "id": "l7J0rqGeC_d4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:212: RuntimeWarning: overflow encountered in reduce\n",
            "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:233: RuntimeWarning: overflow encountered in multiply\n",
            "  x = um.multiply(x, x, out=x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now an error because it's looking at flows trained on catalogs with different keyword names?"
      ],
      "metadata": {
        "id": "op9-IXAOTPmu"
      },
      "id": "op9-IXAOTPmu"
    },
    {
      "cell_type": "markdown",
      "id": "f49e3a2b",
      "metadata": {
        "id": "f49e3a2b"
      },
      "source": [
        "dependencies: pzflow, pandas, jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ab7905",
      "metadata": {
        "id": "83ab7905"
      },
      "outputs": [],
      "source": [
        "from pzflow import Flow\n",
        "import jax.numpy as jnp\n",
        "import pandas as pd \n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a16a1f",
      "metadata": {
        "id": "43a16a1f"
      },
      "outputs": [],
      "source": [
        "from astropy.table import Table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42ed4f28",
      "metadata": {
        "id": "42ed4f28"
      },
      "source": [
        "## CASTOR COSMOS synthetic catalogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff5d07a",
      "metadata": {
        "id": "8ff5d07a"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "unp = Table.read(\"unperturbed_mags.fits\")\n",
        "mock = Table.read(\"ext_phot.fits\")\n",
        "mask = unp[\"Euclid_VIS_MAG\"] <= 24.5\n",
        "\n",
        "CASTOR_baseline = mock[mask] #ignoring training data outside the Euclid flux limit \n",
        "\n",
        "names_phot = [\"ID\", \"photoz\",\n",
        "              \"LSST_g_MAG\", \"LSST_g_MAGERR\", \n",
        "              \"LSST_r_MAG\", \"LSST_r_MAGERR\",\n",
        "              \"LSST_i_MAG\", \"LSST_i_MAGERR\", \n",
        "              \"LSST_z_MAG\", \"LSST_z_MAGERR\"\n",
        "              \"castor_uv_MAG\", \"castor_uv_MAGERR\", \n",
        "              \"castor_u_MAG\", \"castor_u_MAGERR\", \n",
        "              \"castor_g_MAG\", \"castor_g_MAGERR\"]\n",
        "\n",
        "available_os = [\"baseline\"]\n",
        "names = [\n",
        "    \"baseline\",\n",
        "]\n",
        "\n",
        "os_names = dict(zip(available_os, names))\n",
        "colors = [\"k\"] #, \"plum\", \"cornflowerblue\", \"#2ca02c\", \"gold\", \"tomato\"]\n",
        "os_colors = dict(zip(available_os, colors))\n",
        "\n",
        "# put data in expected format for TLM \n",
        "\n",
        "LSST_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAG\")\n",
        "LSST_r_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAG\")\n",
        "LSST_i_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAG\")\n",
        "LSST_z_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAG\")\n",
        "\n",
        "LSST_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAGERR\")\n",
        "LSST_r_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAGERR\")\n",
        "LSST_i_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAGERR\")\n",
        "LSST_z_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAGERR\")\n",
        "\n",
        "CASTOR_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_g_MAG\")\n",
        "CASTOR_r_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_r_MAG\")\n",
        "CASTOR_i_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_i_MAG\")\n",
        "CASTOR_z_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_z_MAG\")\n",
        "\n",
        "CASTOR_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_g_MAGERR\")\n",
        "CASTOR_r_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_r_MAGERR\")\n",
        "CASTOR_i_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_i_MAGERR\")\n",
        "CASTOR_z_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_z_MAGERR\")\n",
        "\n",
        "ID = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"ID\")\n",
        "z_true = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"photoz\")\n",
        "\n",
        "df = pd.DataFrame({'CASTOR_ID': data_array_sorted_CASTOR_wID[:,0] , 'error': data_array_sorted_CASTOR_wID[:,1], \n",
        "                   'g-r': data_array_sorted_CASTOR_wID[:,2] - data_array_sorted_CASTOR_wID[:,3], \n",
        "                   'r-i': data_array_sorted_CASTOR_wID[:,3] - data_array_sorted_CASTOR_wID[:,4], \n",
        "                   'i-z': data_array_sorted_CASTOR_wID[:,4] - data_array_sorted_CASTOR_wID[:,5], \n",
        "                   'true_z': data_array_sorted_CASTOR_wID[:,6]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db5e24e",
      "metadata": {
        "id": "1db5e24e"
      },
      "outputs": [],
      "source": [
        "# load the catalogs\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    \n",
        "#     cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "#                              'LSST_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR, \n",
        "#                              'LSST_r_mag': LSST_r_mag, 'LSST_r_mag_ERR': LSST_r_mag_ERR, \n",
        "#                              'LSST_i_mag': LSST_i_mag, 'LSST_i_mag_ERR': LSST_i_mag_ERR, \n",
        "#                              'LSST_z_mag': LSST_z_mag, 'LSST_z_mag_ERR': LSST_z_mag_ERR, \n",
        "#                              'CASTOR_uv_mag': LSST_uv_mag, 'LSST_uv_mag_ERR': LSST_uv_mag_ERR, \n",
        "#                              'CASTOR_u_mag': LSST_u_mag, 'LSST_u_mag_ERR': LSST_u_mag_ERR, \n",
        "#               'CASTOR_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR})\n",
        "    \n",
        "    # this will need to change to accomodate multiple catalogs, e.g. LSST only or LSST + CASTOR\n",
        "    \n",
        "    cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "                             'r': LSST_r_mag,\n",
        "                             'g-r': LSST_g_mag - LSST_r_mag, \n",
        "                             'r-i': LSST_r_mag - LSST_i_mag, \n",
        "                             'i-z': LSST_i_mag - LSST_z_mag, \n",
        "                             'uv-u': CASTOR_uv_mag - CASTOR_u_mag; \n",
        "                             'u-g': CASTOR_u_mag - CASTOR_g_mag\n",
        "                      })\n",
        "    \n",
        "    catalogs[os] = cat.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2933ee82",
      "metadata": {
        "id": "2933ee82"
      },
      "source": [
        "# Instantiate and Train \n",
        "\n",
        "### Does this run 'as is'?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "4c66cfd3",
      "metadata": {
        "id": "4c66cfd3",
        "outputId": "bc373026-6676-4b17-d742-841f3684112e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-270b6d2f41b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# train the flow on the given learning rate schedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     loss1 = ens.train(cat_train, sample_errs=True,\n\u001b[0;32m---> 51\u001b[0;31m                        \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                        epochs = 100, seed = 123)\n\u001b[1;32m     53\u001b[0m     loss2 = ens.train(cat_train, sample_errs=True,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'adam' is not defined"
          ]
        }
      ],
      "source": [
        "# first I create a bijector chain\n",
        "# the first bijection is a standard scaler - but I'm not actually using it for standard scaling\n",
        "#     I set the mean and std so that it maps the redshift range (0, 3.2) onto (-5, 5), which is \n",
        "#     the domain of the NeuralSplineCoupling\n",
        "# the second bijection is a NeuralSplineCoupling. I told it to expect 6 conditions,\n",
        "#     which will be the r mag and the galaxy colors\n",
        "bijector = Chain(\n",
        "    StandardScaler(np.atleast_1d(1.6), np.atleast_1d(0.32)),\n",
        "    NeuralSplineCoupling(n_conditions=6)\n",
        ")\n",
        "\n",
        "# I set the latent distribution to a Uniform over (-5, 5)\n",
        "# this range was chosen to match the NeuralSplineCoupling domain\n",
        "# I chose a Uniform since all of the redshifts are drawn from a compact domain\n",
        "latent = Uniform(1)\n",
        "\n",
        "# create a dictionary that will hold all the ensembles\n",
        "ensembles = dict()\n",
        "\n",
        "# create the baseline flows\n",
        "for os in available_os:\n",
        "\n",
        "    # the data column is the one that is sampled and transformed by the flow\n",
        "    data_columns = [\"z_true\"]\n",
        "    # the conditional columns are the columns that the flow is conditioned on\n",
        "    conditional_columns = [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\", \"uv-u\"] # different colors than LSST  \n",
        "\n",
        "    # save some info with the flow\n",
        "    info = f\"Models z_true conditioned on galaxy colors and r mag from os {os}. K=16\"\n",
        "\n",
        "    # instantiate and save the flow\n",
        "    flowEns = FlowEnsemble(data_columns = data_columns, \n",
        "                           conditional_columns = conditional_columns,\n",
        "                           bijector = bijector,\n",
        "                           latent = latent,\n",
        "                           info = info,\n",
        "                           N = 10)\n",
        "\n",
        "    ensembles[os] = flowEns\n",
        "    \n",
        "\n",
        "for os, ens in ensembles.items():\n",
        "    \n",
        "    # get the data and make a train and test set\n",
        "    cat = catalogs[os]\n",
        "    cat_train = cat.sample(frac = 0.8)\n",
        "    cat_test = cat.drop(cat_train.index)\n",
        "    \n",
        "    # train the flow on the given learning rate schedule\n",
        "    loss1 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-3),\n",
        "                       epochs = 100, seed = 123)\n",
        "    loss2 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 2e-4),\n",
        "                       epochs = 100, seed = 312)\n",
        "    loss3 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-4),\n",
        "                       epochs = 50, seed = 231)\n",
        "    \n",
        "    losses = {fname : # for each flow trained in the ensemble...\n",
        "                  [float(loss) # save the list of training losses\n",
        "                   for lossDict in [loss1, loss2, loss3]\n",
        "                   for loss in lossDict[fname]]\n",
        "              for fname in loss1}\n",
        "    \n",
        "    # print the train and test loss\n",
        "    train_loss = -np.mean(ens.log_prob(cat_train))\n",
        "    test_loss = -np.mean(ens.log_prob(cat_test))\n",
        "    print(os, train_loss, test_loss)\n",
        "    \n",
        "    # save the ensemble\n",
        "    ens.save(f\"trained_flows/pzflow_ensemble_for_{os}.pkl\")\n",
        "    # and the losses\n",
        "    with open(f\"trained_flows/losses_for_{os}.pkl\", \"wb\") as file:\n",
        "        pickle.dump({\"losses\": losses, \n",
        "                     \"train loss\": train_loss, \n",
        "                     \"test loss\": test_loss},\n",
        "                    file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27fcca35",
      "metadata": {
        "id": "27fcca35"
      },
      "source": [
        "# from github/aimalz/TheLastMetric/blob/master/training_flows_July23.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c392b800",
      "metadata": {
        "id": "c392b800"
      },
      "source": [
        "pull the data just to get formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18074f8",
      "metadata": {
        "id": "f18074f8"
      },
      "outputs": [],
      "source": [
        "# !wget https://storage.googleapis.com/ahw2019/for_malz_and_lanusse.tar.gz\n",
        "# !tar -xzf for_malz_and_lanusse.tar.gz\n",
        "# !mv for_malz_and_lanusse dataset\n",
        "\n",
        "\n",
        "\n",
        "more dataset/readme.txt\n",
        "\n",
        "\n",
        "\n",
        "# list of available catalogs\n",
        "available_os = [\"run_1_4_y10\", \"run_4_38_y10\", \"run_10_92_y10\", \"run_4_34_y10\", \"run_7_61_y10\", \"run_9_86_y10\"]\n",
        "names = [\n",
        "    \"baseline_v1_5_10yrs\",\n",
        "    \"footprint_stuck_rollingv1_5_10yrs\",\n",
        "    \"ddf_heavy_nexp2_v1_6_10yrs\",\n",
        "    \"footprint_newAv1_5_10yrs\",\n",
        "    \"third_obs_pt60v1_5_10yrs\",\n",
        "    \"barebones_v1_6_10yrs\",\n",
        "]\n",
        "os_names = dict(zip(available_os, names))\n",
        "colors = [\"k\", \"plum\", \"cornflowerblue\", \"#2ca02c\", \"gold\", \"tomato\"]\n",
        "os_colors = dict(zip(available_os, colors))\n",
        "\n",
        "\n",
        "# column names of the catalogs\n",
        "names_z=('ID', 'z_true', 'z_phot', 'dz_phot', 'NN', 'N_train')\n",
        "names_phot=(\n",
        "    'ID', 'z_true', \n",
        "    'u', 'u_err',\n",
        "    'g', 'g_err',\n",
        "    'r', 'r_err',\n",
        "    'i', 'i_err',\n",
        "    'z', 'z_err',\n",
        "    'y', 'y_err',\n",
        "    'u-g', 'u-g_err',\n",
        "    'g-r', 'g-r_err',\n",
        "    'r-i', 'r-i_err',\n",
        "    'i-z', 'i-z_err',\n",
        "    'z-y', 'z-y_err',\n",
        ")\n",
        "\n",
        "# load the catalogs\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    z_cat = pd.read_csv(f\"dataset/{os}/zphot.cat\", names=names_z, delim_whitespace=True, skiprows=1)\n",
        "    phot_cat = pd.read_csv(f\"dataset/{os}/test.cat\", names=names_phot, delim_whitespace=True)\n",
        "    cat = z_cat.merge(phot_cat)\n",
        "    catalogs[os] = cat.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c19b11f7",
      "metadata": {
        "id": "c19b11f7"
      },
      "source": [
        "instantiate the model of the photometric space, train it on the catalogs for LSST with and without CASTOR, and save it (two versions rather than 6 OSs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8520d4e2",
      "metadata": {
        "id": "8520d4e2"
      },
      "outputs": [],
      "source": [
        "# first I create a bijector chain\n",
        "# the first bijection is a standard scaler - but I'm not actually using it for standard scaling\n",
        "#     I set the mean and std so that it maps the redshift range (0, 3.2) onto (-5, 5), which is \n",
        "#     the domain of the NeuralSplineCoupling\n",
        "# the second bijection is a NeuralSplineCoupling. I told it to expect 6 conditions,\n",
        "#     which will be the r mag and the galaxy colors\n",
        "bijector = Chain(\n",
        "    StandardScaler(np.atleast_1d(1.6), np.atleast_1d(0.32)),\n",
        "    NeuralSplineCoupling(n_conditions=6)\n",
        ")\n",
        "\n",
        "# I set the latent distribution to a Uniform over (-5, 5)\n",
        "# this range was chosen to match the NeuralSplineCoupling domain\n",
        "# I chose a Uniform since all of the redshifts are drawn from a compact domain\n",
        "latent = Uniform((-5, 5))\n",
        "\n",
        "# create a dictionary that will hold all the ensembles\n",
        "ensembles = dict()\n",
        "\n",
        "# create the baseline flows\n",
        "for os in available_os:\n",
        "\n",
        "    # the data column is the one that is sampled and transformed by the flow\n",
        "    data_columns = [\"z_true\"]\n",
        "    # the conditional columns are the columns that the flow is conditioned on\n",
        "    conditional_columns = [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\", \"z-y\"]\n",
        "\n",
        "    # save some info with the flow\n",
        "    info = f\"Models z_true conditioned on galaxy colors and r mag from os {os}. K=16\"\n",
        "\n",
        "    # instantiate and save the flow\n",
        "    flowEns = FlowEnsemble(data_columns = data_columns, \n",
        "                           conditional_columns = conditional_columns,\n",
        "                           bijector = bijector,\n",
        "                           latent = latent,\n",
        "                           info = info,\n",
        "                           N = 10)\n",
        "\n",
        "    ensembles[os] = flowEns\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "%%time\n",
        "\n",
        "for os, ens in ensembles.items():\n",
        "    \n",
        "    # get the data and make a train and test set\n",
        "    cat = catalogs[os]\n",
        "    cat_train = cat.sample(frac = 0.8)\n",
        "    cat_test = cat.drop(cat_train.index)\n",
        "    \n",
        "    # train the flow on the given learning rate schedule\n",
        "    loss1 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-3),\n",
        "                       epochs = 100, seed = 123)\n",
        "    loss2 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 2e-4),\n",
        "                       epochs = 100, seed = 312)\n",
        "    loss3 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-4),\n",
        "                       epochs = 50, seed = 231)\n",
        "    \n",
        "    losses = {fname : # for each flow trained in the ensemble...\n",
        "                  [float(loss) # save the list of training losses\n",
        "                   for lossDict in [loss1, loss2, loss3]\n",
        "                   for loss in lossDict[fname]]\n",
        "              for fname in loss1}\n",
        "    \n",
        "    # print the train and test loss\n",
        "    train_loss = -np.mean(ens.log_prob(cat_train))\n",
        "    test_loss = -np.mean(ens.log_prob(cat_test))\n",
        "    print(os, train_loss, test_loss)\n",
        "    \n",
        "    # save the ensemble\n",
        "    ens.save(f\"trained_flows/pzflow_ensemble_for_{os}.pkl\")\n",
        "    # and the losses\n",
        "    with open(f\"trained_flows/losses_for_{os}.pkl\", \"wb\") as file:\n",
        "        pickle.dump({\"losses\": losses, \n",
        "                     \"train loss\": train_loss, \n",
        "                     \"test loss\": test_loss},\n",
        "                    file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab4b8090",
      "metadata": {
        "id": "ab4b8090"
      },
      "source": [
        "# from github/aimalz/TheLastMetric/blob/master/MAFVariationalMutualInformationPzFlow.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d935862c",
      "metadata": {
        "id": "d935862c"
      },
      "source": [
        "open the models of redshift-photometry space and the catalogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f28566",
      "metadata": {
        "id": "35f28566"
      },
      "outputs": [],
      "source": [
        "flows = {}\n",
        "for os in available_os:\n",
        "  flows[os] = Flow(file=f\"trained_flows/flow_for_run_{os}.pkl\")\n",
        "# TODO: need to experiment with different fit parameters because this might be too smooth, also does it account for photometric errors?\n",
        "# TODO: check that draws from flow look like original data\n",
        "\n",
        "\n",
        "# load the catalogs\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    z_cat = pd.read_csv(f\"dataset/run_{os}/zphot.cat\", names=names_z, delim_whitespace=True, skiprows=1)\n",
        "    phot_cat = pd.read_csv(f\"dataset/run_{os}/test.cat\", names=names_phot, delim_whitespace=True)\n",
        "    cat = z_cat.merge(phot_cat)\n",
        "    catalogs[os] = cat.dropna()\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e77d45",
      "metadata": {
        "id": "15e77d45"
      },
      "source": [
        "evaluate the posteriors for each galaxy (totally forgot we had to do this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197e07f1",
      "metadata": {
        "id": "197e07f1"
      },
      "outputs": [],
      "source": [
        "# # this just makes the posteriors for plotting, not sure why it uses so much memory. . .\n",
        "# tx = np.linspace(0,3.5,100)\n",
        "# all_logp = {}\n",
        "# for which_os in available_os:\n",
        "#   flow = flows[which_os]\n",
        "#   cat = catalogs[which_os]\n",
        "#   logp = flow.posterior(flow.info[\"condition_scaler\"](cat), column=\"z_true\", grid=tx)\n",
        "#   all_logp[which_os] = logp\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff727de",
      "metadata": {
        "id": "5ff727de"
      },
      "outputs": [],
      "source": [
        "all_milb = {}\n",
        "for which_os in available_os:\n",
        "  phot_cat = catalogs[which_os]\n",
        "\n",
        "  mutual_information_lower_bound = flows[which_os].log_prob(flows[which_os].info[\"condition_scaler\"](phot_cat))\n",
        "  all_milb[which_os] = mutual_information_lower_bound\n",
        "  print((os_names[which_os], np.sum(mutual_information_lower_bound)))\n",
        "# TODO: make this an actual expected value rather than just sum\n",
        "# also, shouldn't it be sum of exponential of metric value, since it should never penalize a negative value?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bde34f5",
      "metadata": {
        "id": "7bde34f5"
      },
      "source": [
        "KEY PLOT: distribution of metric values for same galaxies with and without CASTOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d65b56f",
      "metadata": {
        "id": "6d65b56f"
      },
      "outputs": [],
      "source": [
        "# surprisingly not so different from one another\n",
        "for which_os in available_os:\n",
        "  mutual_information_lower_bound = all_milb[which_os].flatten()\n",
        "  print((np.mean(mutual_information_lower_bound), np.std(mutual_information_lower_bound)))\n",
        "  hist(mutual_information_lower_bound, bins=np.linspace(-16, 5, 100), alpha=0.5, histtype='step', \n",
        "       color=os_colors[which_os], label=os_names[which_os], density=False)\n",
        "  xlabel(r'$\\mathbb{E}_{z, x_{phot}} \\left[ q_\\theta(z | x_{phot}) \\right]$')\n",
        "# xlim(-10., 5.)\n",
        "legend(loc='upper left')\n",
        "# semilogy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca64b1a0",
      "metadata": {
        "id": "ca64b1a0"
      },
      "source": [
        "KEY PLOT: look at per-galaxy metric values as a function of redshift, then photometry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b500416",
      "metadata": {
        "id": "6b500416"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(len(available_os), 1, figsize=(5, 5*len(available_os)))\n",
        "for i, which_os in enumerate(available_os):\n",
        "  axs[i].hist2d(z_cats[which_os]['z_true'], all_milb[which_os].flatten(), bins=[np.linspace(0., 3., 50), np.linspace(-5., 5., 100)])\n",
        "  axs[i].set_xlabel('redshift')\n",
        "  axs[i].set_ylabel(r'$\\mathbb{E}_{z, x_{phot}} \\left[ q_\\theta(z | x_{phot}) \\right]$')\n",
        "  axs[i].set_title(os_names[which_os])\n",
        "# they're different, but not visibly so\n",
        "# TODO: plot violins of metric as a function of binned redshift so they're all on one set of axes? or quantiles because outlers? or box/whisker https://matplotlib.org/stable/gallery/pyplots/boxplot_demo_pyplot.html?\n",
        "# TODO: normalize within redshift bins to get these on one set of axes?\n",
        "# TODO sort of want sum of metric values in redshift bins, no?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680f3fbc",
      "metadata": {
        "id": "680f3fbc"
      },
      "source": [
        "take expected value of the approximated posteriors, which is the mutual information lower bound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c46d85",
      "metadata": {
        "id": "c0c46d85"
      },
      "outputs": [],
      "source": [
        "# something isn't right about the autocalculation of moments so doing it by hand\n",
        "def calc_moment(vals, k):\n",
        "  n = len(vals)\n",
        "  outval = np.sum(vals**k) / float(n)\n",
        "  return float(outval)\n",
        "\n",
        "which_moments = range(0, 5)\n",
        "moment_res = {}\n",
        "for which_os in available_os:\n",
        "  # print((np.mean(all_milb[which_os]), np.std(all_milb[which_os])))\n",
        "  moment_res[which_os] = []\n",
        "  for i in which_moments:\n",
        "    moment_res[which_os].append(calc_moment(all_milb[which_os], k=i))#sps.mstats.moment(all_milb[which_os], moment=which_moments[i], axis=0))\n",
        "# print(moment_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a67574d",
      "metadata": {
        "id": "1a67574d"
      },
      "source": [
        "# from github/aimalz/TheLastMetric/blob/master/FigureTLMvsRedshift.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea32a245",
      "metadata": {
        "id": "ea32a245"
      },
      "source": [
        "KEY PLOT: distribution of TLM values with and without CASTOR including model uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f064b1",
      "metadata": {
        "id": "c9f064b1"
      },
      "outputs": [],
      "source": [
        "%pylab inline\n",
        "from utils import load_data, compute_last_metric\n",
        "import corner\n",
        "from pzflow import Flow\n",
        "\n",
        "\n",
        "# Loading data\n",
        "z_cats, phot_cats, available_os, os_names, os_colors = load_data()\n",
        "\n",
        "\n",
        "# Loading pre-trained flows\n",
        "flows = {}\n",
        "for os in available_os:\n",
        "  flows[os] = [Flow(file=f\"trained_flows/flow_for_run_{os}_%d.pkl\"%(i+1) ) for i in range(10)]\n",
        "\n",
        "\n",
        "# Computing metric for each observing strategy\n",
        "all_tlm = {}\n",
        "for which_os in available_os:\n",
        "  all_tlm[which_os] = np.stack([(compute_last_metric(f,\n",
        "                                          phot_cats[which_os],\n",
        "                                          z_cats[which_os], entropy_nbins=60)) for f in flows[which_os] ], axis=0)\n",
        "  print((os_names[which_os], np.mean(all_tlm[which_os]), np.std(np.mean(all_tlm[which_os], axis=1))))\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "figure(figsize=(7,5))\n",
        "for which_os in available_os:\n",
        "  hist(np.mean(all_tlm[which_os],axis=1), 32, range=[2.95, 3.35], alpha=0.6,\n",
        "       color=os_colors[which_os], label=os_names[which_os])\n",
        "  axvline(np.mean(all_tlm[which_os]), color=os_colors[which_os], linewidth=2)\n",
        "legend(ncol=2)\n",
        "xlabel(chr(0x05ea))\n",
        "savefig('metrics.pdf', bbox_inches = 'tight', pad_inches = 0 )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}