{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aimalz/CASTORpz/blob/main/TheLastMetric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "P9ScjvW6K--M",
        "outputId": "67301523-af90-49ce-a25f-a3b7d5d734ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "P9ScjvW6K--M",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Research/CASTORpz"
      ],
      "metadata": {
        "id": "AIt3xNrROef2",
        "outputId": "0df683a9-42f3-460f-f030-0e90601d550d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AIt3xNrROef2",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ext_phot.fits  knn_colors.py  unperturbed_mags.fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from astropy.table import Table\n",
        "prepend = '/content/drive/MyDrive/Research/CASTORpz/'\n",
        "unp = Table.read(prepend+\"unperturbed_mags.fits\")\n",
        "print(len(unp))"
      ],
      "metadata": {
        "id": "tpLREN_EM6hg",
        "outputId": "3abe92f1-eb3e-4834-c1ed-f21c8ddf544c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tpLREN_EM6hg",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "698211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install astropy pzflow"
      ],
      "metadata": {
        "id": "3u5XpOo5PdZF",
        "outputId": "40dadf94-2a59-4059-92f6-0799d2f34fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3u5XpOo5PdZF",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pzflow\n",
            "  Downloading pzflow-3.0.3-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxlib<0.5.0,>=0.4.2\n",
            "  Downloading jaxlib-0.4.2-cp38-cp38-manylinux2014_x86_64.whl (71.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill<0.4.0,>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from pzflow) (0.3.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.64.1 in /usr/local/lib/python3.8/dist-packages (from pzflow) (4.64.1)\n",
            "Collecting optax<0.2.0,>=0.1.4\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jax<0.5.0,>=0.4.2\n",
            "  Downloading jax-0.4.2.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.8/dist-packages (from pzflow) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax<0.5.0,>=0.4.2->pzflow) (1.21.6)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.8/dist-packages (from jax<0.5.0,>=0.4.2->pzflow) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax<0.5.0,>=0.4.2->pzflow) (1.7.3)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax<0.2.0,>=0.1.4->pzflow) (1.4.0)\n",
            "Collecting chex>=0.1.5\n",
            "  Downloading chex-0.1.6-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from optax<0.2.0,>=0.1.4->pzflow) (4.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1->pzflow) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1->pzflow) (2022.7.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax<0.2.0,>=0.1.4->pzflow) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax<0.2.0,>=0.1.4->pzflow) (0.12.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1->pzflow) (1.15.0)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.2-py3-none-any.whl size=1363025 sha256=591170a27fb119a39871e5fe4b0c4aeb00fb59a292c975d93294849eef9c55e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/34/dd/2150fc0a0285435a6996e78998f124051996ef139e5ab7b310\n",
            "Successfully built jax\n",
            "Installing collected packages: jaxlib, jax, chex, optax, pzflow\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.25+cuda11.cudnn805\n",
            "    Uninstalling jaxlib-0.3.25+cuda11.cudnn805:\n",
            "      Successfully uninstalled jaxlib-0.3.25+cuda11.cudnn805\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.3.25\n",
            "    Uninstalling jax-0.3.25:\n",
            "      Successfully uninstalled jax-0.3.25\n",
            "Successfully installed chex-0.1.6 jax-0.4.2 jaxlib-0.4.2 optax-0.1.4 pzflow-3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pzflow import Flow\n",
        "import jax.numpy as jnp\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from astropy.table import Table\n",
        "from pzflow import Flow, FlowEnsemble\n",
        "from pzflow.distributions import Uniform\n",
        "from pzflow.bijectors import Chain, StandardScaler, NeuralSplineCoupling"
      ],
      "metadata": {
        "id": "A_-243XtQsTp"
      },
      "id": "A_-243XtQsTp",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "def getTrueY(test_cat, mag_col_names, y_col_name):\n",
        "    test_cat = Table(test_cat, masked=True, copy=True)\n",
        "    # remove nans\n",
        "    for col in mag_col_names:\n",
        "        test_cat[col].mask = np.isnan(test_cat[col].data) | test_cat[col].mask\n",
        "        test_cat = test_cat[~test_cat[col].mask] # then remove nans from test set\n",
        "            \n",
        "    true_y = test_cat[y_col_name]\n",
        "    return true_y.filled()"
      ],
      "metadata": {
        "id": "bOIAJea1Q8sv"
      },
      "id": "bOIAJea1Q8sv",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unp = Table.read(prepend+\"unperturbed_mags.fits\")\n",
        "mock = Table.read(prepend+\"ext_phot.fits\")\n",
        "mask = unp[\"Euclid_VIS_MAG\"] <= 24.5"
      ],
      "metadata": {
        "id": "lxPsZxjmRAqb"
      },
      "id": "lxPsZxjmRAqb",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CASTOR_baseline = mock[mask] #ignoring training data outside the Euclid flux limit "
      ],
      "metadata": {
        "id": "KlRbMi9lRFcl"
      },
      "id": "KlRbMi9lRFcl",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_phot = [\"ID\", \"photoz\",\n",
        "              \"LSST_g_MAG\", \"LSST_g_MAGERR\", \n",
        "              \"LSST_r_MAG\", \"LSST_r_MAGERR\",\n",
        "              \"LSST_i_MAG\", \"LSST_i_MAGERR\", \n",
        "              \"LSST_z_MAG\", \"LSST_z_MAGERR\",\n",
        "              \"castor_uv_MAG\", \"castor_uv_MAGERR\", \n",
        "              \"castor_u_MAG\", \"castor_u_MAGERR\", \n",
        "              \"castor_g_MAG\", \"castor_g_MAGERR\"]\n",
        "available_os = [\"baseline\"]\n",
        "names = [\n",
        "    \"baseline\",\n",
        "]"
      ],
      "metadata": {
        "id": "40UyfzNgRHtr"
      },
      "id": "40UyfzNgRHtr",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os_names = dict(zip(available_os, names))\n",
        "colors = [\"k\"] #, \"plum\", \"cornflowerblue\", \"#2ca02c\", \"gold\", \"tomato\"]\n",
        "os_colors = dict(zip(available_os, colors))"
      ],
      "metadata": {
        "id": "7_ElTN-FR281"
      },
      "id": "7_ElTN-FR281",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# put data in expected format for TLM\n",
        "\n",
        "LSST_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAG\")\n",
        "LSST_r_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAG\")\n",
        "LSST_i_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAG\")\n",
        "LSST_z_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAG\")\n",
        "\n",
        "LSST_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAGERR\")\n",
        "LSST_r_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAGERR\")\n",
        "LSST_i_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAGERR\")\n",
        "LSST_z_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAGERR\")\n",
        "\n",
        "CASTOR_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_g_MAG\")\n",
        "CASTOR_u_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_u_MAG\")\n",
        "CASTOR_uv_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_uv_MAG\")\n",
        "\n",
        "CASTOR_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_g_MAGERR\")\n",
        "CASTOR_u_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_u_MAGERR\")\n",
        "CASTOR_uv_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_uv_MAGERR\")\n",
        "\n",
        "ID = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"ID\")\n",
        "z_true = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"photoz\")\n",
        "\n"
      ],
      "metadata": {
        "id": "f8fFRKHGR7nR"
      },
      "id": "f8fFRKHGR7nR",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.DataFrame({'CASTOR_ID': data_array_sorted_CASTOR_wID[:,0] , 'error': data_array_sorted_CASTOR_wID[:,1], \n",
        "#                   'g-r': data_array_sorted_CASTOR_wID[:,2] - data_array_sorted_CASTOR_wID[:,3], \n",
        "#                   'r-i': data_array_sorted_CASTOR_wID[:,3] - data_array_sorted_CASTOR_wID[:,4], \n",
        "#                   'i-z': data_array_sorted_CASTOR_wID[:,4] - data_array_sorted_CASTOR_wID[:,5], \n",
        "#                   'true_z': data_array_sorted_CASTOR_wID[:,6]})\n",
        "\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    \n",
        "#     cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "#                              'LSST_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR, \n",
        "#                              'LSST_r_mag': LSST_r_mag, 'LSST_r_mag_ERR': LSST_r_mag_ERR, \n",
        "#                              'LSST_i_mag': LSST_i_mag, 'LSST_i_mag_ERR': LSST_i_mag_ERR, \n",
        "#                              'LSST_z_mag': LSST_z_mag, 'LSST_z_mag_ERR': LSST_z_mag_ERR, \n",
        "#                              'CASTOR_uv_mag': LSST_uv_mag, 'LSST_uv_mag_ERR': LSST_uv_mag_ERR, \n",
        "#                              'CASTOR_u_mag': LSST_u_mag, 'LSST_u_mag_ERR': LSST_u_mag_ERR, \n",
        "#               'CASTOR_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR})\n",
        "    \n",
        "    # this will need to change to accomodate multiple catalogs, e.g. LSST only or LSST + CASTOR\n",
        "    \n",
        "    cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "                             'r': LSST_r_mag,\n",
        "                             'g-r': LSST_g_mag - LSST_r_mag, \n",
        "                             'r-i': LSST_r_mag - LSST_i_mag, \n",
        "                             'i-z': LSST_i_mag - LSST_z_mag, \n",
        "                             'uv-u': CASTOR_uv_mag - CASTOR_u_mag, \n",
        "                             'u-g': CASTOR_u_mag - CASTOR_g_mag\n",
        "                      })\n",
        "    catalogs[os] = cat.dropna()"
      ],
      "metadata": {
        "id": "NhgJne8VSQuZ",
        "outputId": "35e8de43-b532-429e-ea14-fc041cd9fd08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "id": "NhgJne8VSQuZ",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-db25b8cfd7b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                              \u001b[0;34m'u-g'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCASTOR_u_mag\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mCASTOR_g_mag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                       })\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mcatalogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdropna\u001b[0;34m(self, axis, how, thresh, subset, inplace)\u001b[0m\n\u001b[1;32m   5968\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"must specify how or thresh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5970\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5972\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3627\u001b[0m         \"\"\"\n\u001b[0;32m-> 3628\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3629\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3613\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3615\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   3616\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3617\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m         return self.reindex_indexer(\n\u001b[0m\u001b[1;32m    866\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m    678\u001b[0m             )\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             new_blocks = [\n\u001b[0m\u001b[1;32m    681\u001b[0m                 blk.take_nd(\n\u001b[1;32m    682\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             new_blocks = [\n\u001b[0;32m--> 681\u001b[0;31m                 blk.take_nd(\n\u001b[0m\u001b[1;32m    682\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0mallow_fill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m         new_values = algos.take_nd(\n\u001b[0m\u001b[1;32m   1145\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_take_nd_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/algos_take_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.algos.take_2d_axis0_int64_int64\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Big-endian buffer not supported on little-endian compiler"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still getting error here:\n",
        "\n",
        "\n",
        "```\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "ValueError                                Traceback (most recent call last)\n",
        "\n",
        "<ipython-input-33-db25b8cfd7b0> in <module>\n",
        "     27                              'u-g': CASTOR_u_mag - CASTOR_g_mag\n",
        "     28                       })\n",
        "---> 29     catalogs[os] = cat.dropna()\n",
        "\n",
        "12 frames\n",
        "\n",
        "/usr/local/lib/python3.8/dist-packages/pandas/core/array_algos/take.py in _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
        "    152         arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n",
        "    153     )\n",
        "--> 154     func(arr, indexer, out, fill_value)\n",
        "    155 \n",
        "    156     if flip_order:\n",
        "\n",
        "pandas/_libs/algos_take_helper.pxi in pandas._libs.algos.take_2d_axis0_int64_int64()\n",
        "\n",
        "ValueError: Big-endian buffer not supported on little-endian compiler\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "T0MYBOBuSe-t"
      },
      "id": "T0MYBOBuSe-t"
    },
    {
      "cell_type": "code",
      "source": [
        "# first I create a bijector chain\n",
        "# the first bijection is a standard scaler - but I'm not actually using it for standard scaling\n",
        "#     I set the mean and std so that it maps the redshift range (0, 3.2) onto (-5, 5), which is \n",
        "#     the domain of the NeuralSplineCoupling\n",
        "# the second bijection is a NeuralSplineCoupling. I told it to expect 6 conditions,\n",
        "#     which will be the r mag and the galaxy colors\n",
        "bijector = Chain(\n",
        "    StandardScaler(np.atleast_1d(1.6), np.atleast_1d(0.32)),\n",
        "    NeuralSplineCoupling(n_conditions=6)\n",
        ")\n",
        "\n",
        "# I set the latent distribution to a Uniform over (-5, 5)\n",
        "# this range was chosen to match the NeuralSplineCoupling domain\n",
        "# I chose a Uniform since all of the redshifts are drawn from a compact domain\n",
        "latent = Uniform(1, 5) # did the syntax here change?"
      ],
      "metadata": {
        "id": "CcBqdH2CSpjY"
      },
      "id": "CcBqdH2CSpjY",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary that will hold all the ensembles\n",
        "ensembles = dict()\n",
        "\n",
        "# create the baseline flows\n",
        "for os in available_os:\n",
        "    # the data column is the one that is sampled and transformed by the flow\n",
        "    data_columns = [\"z_true\"]\n",
        "    # the conditional columns are the columns that the flow is conditioned on\n",
        "    conditional_columns = [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\", \"uv - u\"] # different colors than LSST  \n",
        "    # save some info with the flow\n",
        "    info = f\"Models z_true conditioned on galaxy colors and r mag from os {os}. K=16\"\n",
        "    \n",
        "    # instantiate and save the flow\n",
        "    flowEns = FlowEnsemble(data_columns = data_columns, \n",
        "                           conditional_columns = conditional_columns,\n",
        "                           bijector = bijector,\n",
        "                           latent = latent,\n",
        "                           info = info,\n",
        "                           N = 10)\n",
        "    ensembles[os] = flowEns"
      ],
      "metadata": {
        "id": "LQUPqdiGSzyW",
        "outputId": "049c9a1a-5ac0-4524-8375-7b80800b7d57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LQUPqdiGSzyW",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for os, ens in ensembles.items():\n",
        "    \n",
        "    # get the data and make a train and test set\n",
        "    cat = catalogs[os]\n",
        "    cat_train = cat.sample(frac = 0.8)\n",
        "    cat_test = cat.drop(cat_train.index)\n",
        "    \n",
        "    # train the flow on the given learning rate schedule\n",
        "    loss1 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-3),\n",
        "                       epochs = 100, seed = 123)\n",
        "    loss2 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 2e-4),\n",
        "                       epochs = 100, seed = 312)\n",
        "    loss3 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-4),\n",
        "                       epochs = 50, seed = 231)\n",
        "    \n",
        "    losses = {fname : # for each flow trained in the ensemble...\n",
        "                  [float(loss) # save the list of training losses\n",
        "                   for lossDict in [loss1, loss2, loss3]\n",
        "                   for loss in lossDict[fname]]\n",
        "              for fname in loss1}\n",
        "    \n",
        "    # print the train and test loss\n",
        "    train_loss = -np.mean(ens.log_prob(cat_train))\n",
        "    test_loss = -np.mean(ens.log_prob(cat_test))\n",
        "    print(os, train_loss, test_loss)\n",
        "    \n",
        "    # save the ensemble\n",
        "    ens.save(f\"trained_flows/pzflow_ensemble_for_{os}.pkl\")\n",
        "    # and the losses\n",
        "    with open(f\"trained_flows/losses_for_{os}.pkl\", \"wb\") as file:\n",
        "        pickle.dump({\"losses\": losses, \n",
        "                     \"train loss\": train_loss, \n",
        "                     \"test loss\": test_loss},\n",
        "                    file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "l7J0rqGeC_d4",
        "outputId": "33ce94f1-4cd2-4ba7-c38c-bd1ab12f8f3c"
      },
      "id": "l7J0rqGeC_d4",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3e74bac6139d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# get the data and make a train and test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatalogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcat_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcat_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'baseline'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now an error because it's looking at flows trained on catalogs with different keyword names?"
      ],
      "metadata": {
        "id": "op9-IXAOTPmu"
      },
      "id": "op9-IXAOTPmu"
    },
    {
      "cell_type": "markdown",
      "id": "f49e3a2b",
      "metadata": {
        "id": "f49e3a2b"
      },
      "source": [
        "dependencies: pzflow, pandas, jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ab7905",
      "metadata": {
        "id": "83ab7905"
      },
      "outputs": [],
      "source": [
        "from pzflow import Flow\n",
        "import jax.numpy as jnp\n",
        "import pandas as pd \n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a16a1f",
      "metadata": {
        "id": "43a16a1f"
      },
      "outputs": [],
      "source": [
        "from astropy.table import Table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42ed4f28",
      "metadata": {
        "id": "42ed4f28"
      },
      "source": [
        "## CASTOR COSMOS synthetic catalogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff5d07a",
      "metadata": {
        "id": "8ff5d07a"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "unp = Table.read(\"unperturbed_mags.fits\")\n",
        "mock = Table.read(\"ext_phot.fits\")\n",
        "mask = unp[\"Euclid_VIS_MAG\"] <= 24.5\n",
        "\n",
        "CASTOR_baseline = mock[mask] #ignoring training data outside the Euclid flux limit \n",
        "\n",
        "names_phot = [\"ID\", \"photoz\",\n",
        "              \"LSST_g_MAG\", \"LSST_g_MAGERR\", \n",
        "              \"LSST_r_MAG\", \"LSST_r_MAGERR\",\n",
        "              \"LSST_i_MAG\", \"LSST_i_MAGERR\", \n",
        "              \"LSST_z_MAG\", \"LSST_z_MAGERR\"\n",
        "              \"castor_uv_MAG\", \"castor_uv_MAGERR\", \n",
        "              \"castor_u_MAG\", \"castor_u_MAGERR\", \n",
        "              \"castor_g_MAG\", \"castor_g_MAGERR\"]\n",
        "\n",
        "available_os = [\"baseline\"]\n",
        "names = [\n",
        "    \"baseline\",\n",
        "]\n",
        "\n",
        "os_names = dict(zip(available_os, names))\n",
        "colors = [\"k\"] #, \"plum\", \"cornflowerblue\", \"#2ca02c\", \"gold\", \"tomato\"]\n",
        "os_colors = dict(zip(available_os, colors))\n",
        "\n",
        "# put data in expected format for TLM \n",
        "\n",
        "LSST_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAG\")\n",
        "LSST_r_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAG\")\n",
        "LSST_i_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAG\")\n",
        "LSST_z_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAG\")\n",
        "\n",
        "LSST_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAGERR\")\n",
        "LSST_r_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAGERR\")\n",
        "LSST_i_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAGERR\")\n",
        "LSST_z_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAGERR\")\n",
        "\n",
        "CASTOR_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_g_MAG\")\n",
        "CASTOR_r_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_r_MAG\")\n",
        "CASTOR_i_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_i_MAG\")\n",
        "CASTOR_z_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_z_MAG\")\n",
        "\n",
        "CASTOR_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_g_MAGERR\")\n",
        "CASTOR_r_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_r_MAGERR\")\n",
        "CASTOR_i_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_i_MAGERR\")\n",
        "CASTOR_z_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_z_MAGERR\")\n",
        "\n",
        "ID = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"ID\")\n",
        "z_true = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"photoz\")\n",
        "\n",
        "df = pd.DataFrame({'CASTOR_ID': data_array_sorted_CASTOR_wID[:,0] , 'error': data_array_sorted_CASTOR_wID[:,1], \n",
        "                   'g-r': data_array_sorted_CASTOR_wID[:,2] - data_array_sorted_CASTOR_wID[:,3], \n",
        "                   'r-i': data_array_sorted_CASTOR_wID[:,3] - data_array_sorted_CASTOR_wID[:,4], \n",
        "                   'i-z': data_array_sorted_CASTOR_wID[:,4] - data_array_sorted_CASTOR_wID[:,5], \n",
        "                   'true_z': data_array_sorted_CASTOR_wID[:,6]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db5e24e",
      "metadata": {
        "id": "1db5e24e"
      },
      "outputs": [],
      "source": [
        "# load the catalogs\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    \n",
        "#     cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "#                              'LSST_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR, \n",
        "#                              'LSST_r_mag': LSST_r_mag, 'LSST_r_mag_ERR': LSST_r_mag_ERR, \n",
        "#                              'LSST_i_mag': LSST_i_mag, 'LSST_i_mag_ERR': LSST_i_mag_ERR, \n",
        "#                              'LSST_z_mag': LSST_z_mag, 'LSST_z_mag_ERR': LSST_z_mag_ERR, \n",
        "#                              'CASTOR_uv_mag': LSST_uv_mag, 'LSST_uv_mag_ERR': LSST_uv_mag_ERR, \n",
        "#                              'CASTOR_u_mag': LSST_u_mag, 'LSST_u_mag_ERR': LSST_u_mag_ERR, \n",
        "#               'CASTOR_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR})\n",
        "    \n",
        "    # this will need to change to accomodate multiple catalogs, e.g. LSST only or LSST + CASTOR\n",
        "    \n",
        "    cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "                             'r': LSST_r_mag,\n",
        "                             'g-r': LSST_g_mag - LSST_r_mag, \n",
        "                             'r-i': LSST_r_mag - LSST_i_mag, \n",
        "                             'i-z': LSST_i_mag - LSST_z_mag, \n",
        "                             'uv-u': CASTOR_uv_mag - CASTOR_u_mag; \n",
        "                             'u-g': CASTOR_u_mag - CASTOR_g_mag\n",
        "                      })\n",
        "    \n",
        "    catalogs[os] = cat.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2933ee82",
      "metadata": {
        "id": "2933ee82"
      },
      "source": [
        "# Instantiate and Train \n",
        "\n",
        "### Does this run 'as is'?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c66cfd3",
      "metadata": {
        "id": "4c66cfd3"
      },
      "outputs": [],
      "source": [
        "# first I create a bijector chain\n",
        "# the first bijection is a standard scaler - but I'm not actually using it for standard scaling\n",
        "#     I set the mean and std so that it maps the redshift range (0, 3.2) onto (-5, 5), which is \n",
        "#     the domain of the NeuralSplineCoupling\n",
        "# the second bijection is a NeuralSplineCoupling. I told it to expect 6 conditions,\n",
        "#     which will be the r mag and the galaxy colors\n",
        "bijector = Chain(\n",
        "    StandardScaler(np.atleast_1d(1.6), np.atleast_1d(0.32)),\n",
        "    NeuralSplineCoupling(n_conditions=6)\n",
        ")\n",
        "\n",
        "# I set the latent distribution to a Uniform over (-5, 5)\n",
        "# this range was chosen to match the NeuralSplineCoupling domain\n",
        "# I chose a Uniform since all of the redshifts are drawn from a compact domain\n",
        "latent = Uniform((-5, 5))\n",
        "\n",
        "# create a dictionary that will hold all the ensembles\n",
        "ensembles = dict()\n",
        "\n",
        "# create the baseline flows\n",
        "for os in available_os:\n",
        "\n",
        "    # the data column is the one that is sampled and transformed by the flow\n",
        "    data_columns = [\"z_true\"]\n",
        "    # the conditional columns are the columns that the flow is conditioned on\n",
        "    conditional_columns = [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\", \"uv - u\"] # different colors than LSST  \n",
        "\n",
        "    # save some info with the flow\n",
        "    info = f\"Models z_true conditioned on galaxy colors and r mag from os {os}. K=16\"\n",
        "\n",
        "    # instantiate and save the flow\n",
        "    flowEns = FlowEnsemble(data_columns = data_columns, \n",
        "                           conditional_columns = conditional_columns,\n",
        "                           bijector = bijector,\n",
        "                           latent = latent,\n",
        "                           info = info,\n",
        "                           N = 10)\n",
        "\n",
        "    ensembles[os] = flowEns\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "%%time\n",
        "\n",
        "for os, ens in ensembles.items():\n",
        "    \n",
        "    # get the data and make a train and test set\n",
        "    cat = catalogs[os]\n",
        "    cat_train = cat.sample(frac = 0.8)\n",
        "    cat_test = cat.drop(cat_train.index)\n",
        "    \n",
        "    # train the flow on the given learning rate schedule\n",
        "    loss1 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-3),\n",
        "                       epochs = 100, seed = 123)\n",
        "    loss2 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 2e-4),\n",
        "                       epochs = 100, seed = 312)\n",
        "    loss3 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-4),\n",
        "                       epochs = 50, seed = 231)\n",
        "    \n",
        "    losses = {fname : # for each flow trained in the ensemble...\n",
        "                  [float(loss) # save the list of training losses\n",
        "                   for lossDict in [loss1, loss2, loss3]\n",
        "                   for loss in lossDict[fname]]\n",
        "              for fname in loss1}\n",
        "    \n",
        "    # print the train and test loss\n",
        "    train_loss = -np.mean(ens.log_prob(cat_train))\n",
        "    test_loss = -np.mean(ens.log_prob(cat_test))\n",
        "    print(os, train_loss, test_loss)\n",
        "    \n",
        "    # save the ensemble\n",
        "    ens.save(f\"trained_flows/pzflow_ensemble_for_{os}.pkl\")\n",
        "    # and the losses\n",
        "    with open(f\"trained_flows/losses_for_{os}.pkl\", \"wb\") as file:\n",
        "        pickle.dump({\"losses\": losses, \n",
        "                     \"train loss\": train_loss, \n",
        "                     \"test loss\": test_loss},\n",
        "                    file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27fcca35",
      "metadata": {
        "id": "27fcca35"
      },
      "source": [
        "# from github/aimalz/TheLastMetric/blob/master/training_flows_July23.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c392b800",
      "metadata": {
        "id": "c392b800"
      },
      "source": [
        "pull the data just to get formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18074f8",
      "metadata": {
        "id": "f18074f8"
      },
      "outputs": [],
      "source": [
        "# !wget https://storage.googleapis.com/ahw2019/for_malz_and_lanusse.tar.gz\n",
        "# !tar -xzf for_malz_and_lanusse.tar.gz\n",
        "# !mv for_malz_and_lanusse dataset\n",
        "\n",
        "\n",
        "\n",
        "more dataset/readme.txt\n",
        "\n",
        "\n",
        "\n",
        "# list of available catalogs\n",
        "available_os = [\"run_1_4_y10\", \"run_4_38_y10\", \"run_10_92_y10\", \"run_4_34_y10\", \"run_7_61_y10\", \"run_9_86_y10\"]\n",
        "names = [\n",
        "    \"baseline_v1_5_10yrs\",\n",
        "    \"footprint_stuck_rollingv1_5_10yrs\",\n",
        "    \"ddf_heavy_nexp2_v1_6_10yrs\",\n",
        "    \"footprint_newAv1_5_10yrs\",\n",
        "    \"third_obs_pt60v1_5_10yrs\",\n",
        "    \"barebones_v1_6_10yrs\",\n",
        "]\n",
        "os_names = dict(zip(available_os, names))\n",
        "colors = [\"k\", \"plum\", \"cornflowerblue\", \"#2ca02c\", \"gold\", \"tomato\"]\n",
        "os_colors = dict(zip(available_os, colors))\n",
        "\n",
        "\n",
        "# column names of the catalogs\n",
        "names_z=('ID', 'z_true', 'z_phot', 'dz_phot', 'NN', 'N_train')\n",
        "names_phot=(\n",
        "    'ID', 'z_true', \n",
        "    'u', 'u_err',\n",
        "    'g', 'g_err',\n",
        "    'r', 'r_err',\n",
        "    'i', 'i_err',\n",
        "    'z', 'z_err',\n",
        "    'y', 'y_err',\n",
        "    'u-g', 'u-g_err',\n",
        "    'g-r', 'g-r_err',\n",
        "    'r-i', 'r-i_err',\n",
        "    'i-z', 'i-z_err',\n",
        "    'z-y', 'z-y_err',\n",
        ")\n",
        "\n",
        "# load the catalogs\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    z_cat = pd.read_csv(f\"dataset/{os}/zphot.cat\", names=names_z, delim_whitespace=True, skiprows=1)\n",
        "    phot_cat = pd.read_csv(f\"dataset/{os}/test.cat\", names=names_phot, delim_whitespace=True)\n",
        "    cat = z_cat.merge(phot_cat)\n",
        "    catalogs[os] = cat.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c19b11f7",
      "metadata": {
        "id": "c19b11f7"
      },
      "source": [
        "instantiate the model of the photometric space, train it on the catalogs for LSST with and without CASTOR, and save it (two versions rather than 6 OSs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8520d4e2",
      "metadata": {
        "id": "8520d4e2"
      },
      "outputs": [],
      "source": [
        "# first I create a bijector chain\n",
        "# the first bijection is a standard scaler - but I'm not actually using it for standard scaling\n",
        "#     I set the mean and std so that it maps the redshift range (0, 3.2) onto (-5, 5), which is \n",
        "#     the domain of the NeuralSplineCoupling\n",
        "# the second bijection is a NeuralSplineCoupling. I told it to expect 6 conditions,\n",
        "#     which will be the r mag and the galaxy colors\n",
        "bijector = Chain(\n",
        "    StandardScaler(np.atleast_1d(1.6), np.atleast_1d(0.32)),\n",
        "    NeuralSplineCoupling(n_conditions=6)\n",
        ")\n",
        "\n",
        "# I set the latent distribution to a Uniform over (-5, 5)\n",
        "# this range was chosen to match the NeuralSplineCoupling domain\n",
        "# I chose a Uniform since all of the redshifts are drawn from a compact domain\n",
        "latent = Uniform((-5, 5))\n",
        "\n",
        "# create a dictionary that will hold all the ensembles\n",
        "ensembles = dict()\n",
        "\n",
        "# create the baseline flows\n",
        "for os in available_os:\n",
        "\n",
        "    # the data column is the one that is sampled and transformed by the flow\n",
        "    data_columns = [\"z_true\"]\n",
        "    # the conditional columns are the columns that the flow is conditioned on\n",
        "    conditional_columns = [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\", \"z-y\"]\n",
        "\n",
        "    # save some info with the flow\n",
        "    info = f\"Models z_true conditioned on galaxy colors and r mag from os {os}. K=16\"\n",
        "\n",
        "    # instantiate and save the flow\n",
        "    flowEns = FlowEnsemble(data_columns = data_columns, \n",
        "                           conditional_columns = conditional_columns,\n",
        "                           bijector = bijector,\n",
        "                           latent = latent,\n",
        "                           info = info,\n",
        "                           N = 10)\n",
        "\n",
        "    ensembles[os] = flowEns\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "%%time\n",
        "\n",
        "for os, ens in ensembles.items():\n",
        "    \n",
        "    # get the data and make a train and test set\n",
        "    cat = catalogs[os]\n",
        "    cat_train = cat.sample(frac = 0.8)\n",
        "    cat_test = cat.drop(cat_train.index)\n",
        "    \n",
        "    # train the flow on the given learning rate schedule\n",
        "    loss1 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-3),\n",
        "                       epochs = 100, seed = 123)\n",
        "    loss2 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 2e-4),\n",
        "                       epochs = 100, seed = 312)\n",
        "    loss3 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-4),\n",
        "                       epochs = 50, seed = 231)\n",
        "    \n",
        "    losses = {fname : # for each flow trained in the ensemble...\n",
        "                  [float(loss) # save the list of training losses\n",
        "                   for lossDict in [loss1, loss2, loss3]\n",
        "                   for loss in lossDict[fname]]\n",
        "              for fname in loss1}\n",
        "    \n",
        "    # print the train and test loss\n",
        "    train_loss = -np.mean(ens.log_prob(cat_train))\n",
        "    test_loss = -np.mean(ens.log_prob(cat_test))\n",
        "    print(os, train_loss, test_loss)\n",
        "    \n",
        "    # save the ensemble\n",
        "    ens.save(f\"trained_flows/pzflow_ensemble_for_{os}.pkl\")\n",
        "    # and the losses\n",
        "    with open(f\"trained_flows/losses_for_{os}.pkl\", \"wb\") as file:\n",
        "        pickle.dump({\"losses\": losses, \n",
        "                     \"train loss\": train_loss, \n",
        "                     \"test loss\": test_loss},\n",
        "                    file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab4b8090",
      "metadata": {
        "id": "ab4b8090"
      },
      "source": [
        "# from github/aimalz/TheLastMetric/blob/master/MAFVariationalMutualInformationPzFlow.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d935862c",
      "metadata": {
        "id": "d935862c"
      },
      "source": [
        "open the models of redshift-photometry space and the catalogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f28566",
      "metadata": {
        "id": "35f28566"
      },
      "outputs": [],
      "source": [
        "flows = {}\n",
        "for os in available_os:\n",
        "  flows[os] = Flow(file=f\"trained_flows/flow_for_run_{os}.pkl\")\n",
        "# TODO: need to experiment with different fit parameters because this might be too smooth, also does it account for photometric errors?\n",
        "# TODO: check that draws from flow look like original data\n",
        "\n",
        "\n",
        "# load the catalogs\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    z_cat = pd.read_csv(f\"dataset/run_{os}/zphot.cat\", names=names_z, delim_whitespace=True, skiprows=1)\n",
        "    phot_cat = pd.read_csv(f\"dataset/run_{os}/test.cat\", names=names_phot, delim_whitespace=True)\n",
        "    cat = z_cat.merge(phot_cat)\n",
        "    catalogs[os] = cat.dropna()\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e77d45",
      "metadata": {
        "id": "15e77d45"
      },
      "source": [
        "evaluate the posteriors for each galaxy (totally forgot we had to do this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197e07f1",
      "metadata": {
        "id": "197e07f1"
      },
      "outputs": [],
      "source": [
        "# # this just makes the posteriors for plotting, not sure why it uses so much memory. . .\n",
        "# tx = np.linspace(0,3.5,100)\n",
        "# all_logp = {}\n",
        "# for which_os in available_os:\n",
        "#   flow = flows[which_os]\n",
        "#   cat = catalogs[which_os]\n",
        "#   logp = flow.posterior(flow.info[\"condition_scaler\"](cat), column=\"z_true\", grid=tx)\n",
        "#   all_logp[which_os] = logp\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff727de",
      "metadata": {
        "id": "5ff727de"
      },
      "outputs": [],
      "source": [
        "all_milb = {}\n",
        "for which_os in available_os:\n",
        "  phot_cat = catalogs[which_os]\n",
        "\n",
        "  mutual_information_lower_bound = flows[which_os].log_prob(flows[which_os].info[\"condition_scaler\"](phot_cat))\n",
        "  all_milb[which_os] = mutual_information_lower_bound\n",
        "  print((os_names[which_os], np.sum(mutual_information_lower_bound)))\n",
        "# TODO: make this an actual expected value rather than just sum\n",
        "# also, shouldn't it be sum of exponential of metric value, since it should never penalize a negative value?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bde34f5",
      "metadata": {
        "id": "7bde34f5"
      },
      "source": [
        "KEY PLOT: distribution of metric values for same galaxies with and without CASTOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d65b56f",
      "metadata": {
        "id": "6d65b56f"
      },
      "outputs": [],
      "source": [
        "# surprisingly not so different from one another\n",
        "for which_os in available_os:\n",
        "  mutual_information_lower_bound = all_milb[which_os].flatten()\n",
        "  print((np.mean(mutual_information_lower_bound), np.std(mutual_information_lower_bound)))\n",
        "  hist(mutual_information_lower_bound, bins=np.linspace(-16, 5, 100), alpha=0.5, histtype='step', \n",
        "       color=os_colors[which_os], label=os_names[which_os], density=False)\n",
        "  xlabel(r'$\\mathbb{E}_{z, x_{phot}} \\left[ q_\\theta(z | x_{phot}) \\right]$')\n",
        "# xlim(-10., 5.)\n",
        "legend(loc='upper left')\n",
        "# semilogy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca64b1a0",
      "metadata": {
        "id": "ca64b1a0"
      },
      "source": [
        "KEY PLOT: look at per-galaxy metric values as a function of redshift, then photometry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b500416",
      "metadata": {
        "id": "6b500416"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(len(available_os), 1, figsize=(5, 5*len(available_os)))\n",
        "for i, which_os in enumerate(available_os):\n",
        "  axs[i].hist2d(z_cats[which_os]['z_true'], all_milb[which_os].flatten(), bins=[np.linspace(0., 3., 50), np.linspace(-5., 5., 100)])\n",
        "  axs[i].set_xlabel('redshift')\n",
        "  axs[i].set_ylabel(r'$\\mathbb{E}_{z, x_{phot}} \\left[ q_\\theta(z | x_{phot}) \\right]$')\n",
        "  axs[i].set_title(os_names[which_os])\n",
        "# they're different, but not visibly so\n",
        "# TODO: plot violins of metric as a function of binned redshift so they're all on one set of axes? or quantiles because outlers? or box/whisker https://matplotlib.org/stable/gallery/pyplots/boxplot_demo_pyplot.html?\n",
        "# TODO: normalize within redshift bins to get these on one set of axes?\n",
        "# TODO sort of want sum of metric values in redshift bins, no?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680f3fbc",
      "metadata": {
        "id": "680f3fbc"
      },
      "source": [
        "take expected value of the approximated posteriors, which is the mutual information lower bound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c46d85",
      "metadata": {
        "id": "c0c46d85"
      },
      "outputs": [],
      "source": [
        "# something isn't right about the autocalculation of moments so doing it by hand\n",
        "def calc_moment(vals, k):\n",
        "  n = len(vals)\n",
        "  outval = np.sum(vals**k) / float(n)\n",
        "  return float(outval)\n",
        "\n",
        "which_moments = range(0, 5)\n",
        "moment_res = {}\n",
        "for which_os in available_os:\n",
        "  # print((np.mean(all_milb[which_os]), np.std(all_milb[which_os])))\n",
        "  moment_res[which_os] = []\n",
        "  for i in which_moments:\n",
        "    moment_res[which_os].append(calc_moment(all_milb[which_os], k=i))#sps.mstats.moment(all_milb[which_os], moment=which_moments[i], axis=0))\n",
        "# print(moment_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a67574d",
      "metadata": {
        "id": "1a67574d"
      },
      "source": [
        "# from github/aimalz/TheLastMetric/blob/master/FigureTLMvsRedshift.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea32a245",
      "metadata": {
        "id": "ea32a245"
      },
      "source": [
        "KEY PLOT: distribution of TLM values with and without CASTOR including model uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f064b1",
      "metadata": {
        "id": "c9f064b1"
      },
      "outputs": [],
      "source": [
        "%pylab inline\n",
        "from utils import load_data, compute_last_metric\n",
        "import corner\n",
        "from pzflow import Flow\n",
        "\n",
        "\n",
        "# Loading data\n",
        "z_cats, phot_cats, available_os, os_names, os_colors = load_data()\n",
        "\n",
        "\n",
        "# Loading pre-trained flows\n",
        "flows = {}\n",
        "for os in available_os:\n",
        "  flows[os] = [Flow(file=f\"trained_flows/flow_for_run_{os}_%d.pkl\"%(i+1) ) for i in range(10)]\n",
        "\n",
        "\n",
        "# Computing metric for each observing strategy\n",
        "all_tlm = {}\n",
        "for which_os in available_os:\n",
        "  all_tlm[which_os] = np.stack([(compute_last_metric(f,\n",
        "                                          phot_cats[which_os],\n",
        "                                          z_cats[which_os], entropy_nbins=60)) for f in flows[which_os] ], axis=0)\n",
        "  print((os_names[which_os], np.mean(all_tlm[which_os]), np.std(np.mean(all_tlm[which_os], axis=1))))\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "figure(figsize=(7,5))\n",
        "for which_os in available_os:\n",
        "  hist(np.mean(all_tlm[which_os],axis=1), 32, range=[2.95, 3.35], alpha=0.6,\n",
        "       color=os_colors[which_os], label=os_names[which_os])\n",
        "  axvline(np.mean(all_tlm[which_os]), color=os_colors[which_os], linewidth=2)\n",
        "legend(ncol=2)\n",
        "xlabel(chr(0x05ea))\n",
        "savefig('metrics.pdf', bbox_inches = 'tight', pad_inches = 0 )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}